\section{Related Work}


\textbf{Deep learning frameworks}. There are a number of deep learning frameworks~\cite{caffe2,tensorflow,pytorch,tensor2tensor} that provide a composable pipeline for machine learning practitioners to design, train, validate, and deploy deep learning models. Although our measurement study is conducted on the popular TensorFlow framework~\cite{tensorflow}, we believe the results can be extended to other frameworks such as Caffe/FireCaffe, CNTK, MXNet~\cite{caffe2,cntk,mxnet}, etc. The reason is that current deep learning frameworks share the same distributed training method, adopt parameter server to maintain training parameters, use SGD-based methods for optimizing model parameters~\cite{sgd1,stale4}, and support distributed training on multi-GPU servers. However, most of current deep learning frameworks do not natively support elastically adding or removing servers while the training process is ongoing. Very recently, MXNet has embarked the efforts to dynamically scale training jobs on EC2~\cite{dt_mxnet}. Complementary to the recent support of dynamic training, our work pinpoints the need for elasticity in transient distributed training to better utilize the dynamically available transient servers across types, regions and monetary costs. 


\textbf{Performance studies on deep learning.} 
A plethora of works~\cite{2016cloudandbigdata} have compared and studied the deep learning performance under different hardware and software configurations.  
In particular, researchers have investigated the scaling potentials of using CPU servers~\cite{jeffdean}, single GPU and multi-GPU servers~\cite{shi2018performance}.
As the computation need of training deep learning grows and supports for distributed training over a cluster of GPU servers~\cite{project_adam,firecaffe,geeps}, prior work has factored in the impact of network communication~\cite{lin2017deep, wen2017terngrad, strom2015scalable}
and provides initial study on tuning hyperparamters, e.g., learning rate and batch size~\cite{srinivasan2018analysis,stale4,goyal2017accurate,you2018imagenet,akiba2017extremely}, to mitigate the communication bottleneck and impact of stale model parameters~\cite{stale1,stale2,stale3,stale4}. 
However, most works on distributed training performance~\cite{shi2018performance,dl_perf1,dl_perf2} make the implicit assumptions of \emph{static and homogenous} cluster configurations. Our study aims to understand the training performance of leveraging cheap transient servers that have dynamic availability, revocation patterns and unit costs. In addition, these previous studies often focus on measuring the training speed using the average time to process one mini-batch~\cite{2016cloudandbigdata,shi2018performance,peng2018optimus}. while in our paper, we consider important performance metrics, namely training time, cost and accuracy, that could potentially be impacted by training on transient servers. 

\textbf{Performance optimization based on transient servers.} Since transient servers are cheaper than their counterparts, many researchers studied how to effectively running applications on cloud transient servers with as few modifications as possible~\cite{hotspot,spotcheck}. Some researchers proposed transient-aware resource managers~\cite{portfolio-driven,proteus} to optimize job schedulers by taking into account the revocation rates of transient servers. Other researchers proposed system-level fault-tolerance techniques such as dynamic checkpointing to optimize the execution time of various applications including web services~\cite{spotcheck,tributary}, big data applications~\cite{See_spotrun,Flint,Tr-spark,spoton} and other memory-intensive applications~\cite{spot_burstable}. Recently, as training deep learning models can naturally benefit from more resources, DeepSpotCloud~\cite{deepspotcloud} looked at how to effectively run deep learning training by migrating from one GPU server to a cheaper transient server. Our work differs from prior works in two major ways: first, we focus on understanding how distributed training can benefit from cheap transient servers. Unlike previous commonly studied batch jobs, big data applications, or even web services, training deep learning models poses a unique trade-off of converging accuracy and training speed. Second, we explore the feasibility and quantify the benefits of performing distributed training on transient servers and identify important transient-aware design changes in distributed training frameworks in order to more effectively utilize transient resources. 
