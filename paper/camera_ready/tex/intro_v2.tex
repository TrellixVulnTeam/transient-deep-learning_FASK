Distributed training is an attractive solution to the problem of scaling deep
learning to training larger, more complex, and more accurate models. In short,
distributed training allows models to be trained across a cluster of machines
in a fraction of the time it would take to train on a single server. For
example, researchers at Facebook achieved near linear scalability when training
a ResNet-50 model on the ImageNet-1k dataset using 32 GPU-equipped
servers~\cite{}.

Distributed training is especially attractive for companies that want to
leverage cloud-based servers.  All major cloud providers---Google, Microsoft,
and Amazon---all offer GPU server options to support deep learning.  However,
existing distributed training frameworks make traditional assumptions about the
lifetime of cloud servers in its cluster. Namely, that once a server is
acquired by the customer it will remain available until explicitly released
back to the cloud provider by that customer. In this paper, we refer to such
servers as \emph{on-demand}. While this assumption is reasonable for many
deployments, we argue that it also represents a missed opportunity.   

In this work, we ask the question: what if we use \emph{transient} rather than
\emph{on-demand} servers for distributed training.  Transient servers offer
significantly lower costs than their on-demand equivalents with the added
complication that the cloud provider may \emph{revoke} them at anytime---violating the
availability assumption discussed in the preceding paragraph.  Google,
Microsoft, and Amazon all offer transient servers, so the idea of 
distributed training with transient servers is applicable to all three major
cloud platforms

Consider the following motivating experiment. Using a single on-demand GPU
server on Google Compute Engine we were able to train a XX model in 3.91 hours
with a total cost of \$2.83 on average. When we use distributed training with
four on-demand servers---with each machine identical to the single server used 
the previous runs---we decreased the average training time to 0.99 hours
with similar overall cost of \$2.92. Finally, when we use distributed training
with four \emph{transient} servers we retain the improvement in training time,
1.05 hours on average, while significantly reducing the total cost to \$1.05 on
average. We saw these performance increases even though we made no significant
modifications to the distributed training frameworks and 13 of the 132 transient
servers (affecting 11 out of the 32 clusters) were revoked at some point prior
to the completion of training. We provide a more detailed analysis of this
experiment and the impact of server revocation in Section~\ref{??}.      

Our goal is to identify the important design considerations needed for
rearchitecting distributed training frameworks to support transient servers.
While the simple experiment above demonstrates the potential of distributed
training with transient servers (e.g., reduced training time and cost) as well
as the challenges (e.g., server revocation and availability), we believe that
transient servers also offer additional opportunities.  For example, price
dynamics make it more attractive to use clusters with machines drawn from
multiple, geographically-diverse, data centers. Such an approach raises
interesting questions about the impact of communication costs and latency on
training performance. Similarly, rather use a cluster composed of servers all
of the same type,  we might employ heterogeneous clusters composed of machines
with different computational resources and capabilities. Finally, the clusters
themselves need not be static; instead, we might dynamically add or remove
servers to make distributed training more robust to server revocation or to
take advantage of volatile server pricing.   
 

Contributions.
