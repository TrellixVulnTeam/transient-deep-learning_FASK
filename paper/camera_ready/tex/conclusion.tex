\section{Conclusions and Future Work}

In this paper, we described the first large-scale empirical evaluation of
distributed training using transient servers.  We compared various transient
server cluster configurations for training a popular CNN model called
\emph{ResNet-32} with a standard image recognition dataset \emph{Cifar-10}.
Using  training on a single GPU server as a baseline,  we observe up to a 7.7X
training speedup within the same cost budget and with a slight accuracy
decrease---an artifact of asynchronous training that is not caused by the use
of transient servers. In fact, we observe that model accuracy on average is
higher when workers are revoked when compared to distributed training without
revocation. Our observations suggest that deep learning frameworks could better
leverage trade-offs across all three performance metrics---i.e., model training
time, training cost, and accuracy---if cloud providers rework the revocation
mechanism. In addition, our analysis reveals several ways that current training
frameworks can better utilize transient servers, e.g., by offering increased
flexibility for model checkpointing and supporting dynamic scaling.  


%In this paper, we empirically evaluate the feasibility and effectiveness of utilizing cheap GPU servers to perform distributed training. We first present characterizations of transient GPU servers, and the baseline performance of training using single GPU servers. We then present our findings about running different factors that affect distributed training performance, quantified by training speed and accuracy. Last, we propose a number of optimization techniques that can allow us better to utilize geographically distributed transient servers, with adaptive checkpoint frequency, dynamic batch size configuration, and most important of all, adding or removing training servers on demand. 
%
%Our study suggests that utilizing transient servers can significantly improve the training speed for training medium-sized DNN models because these models are more likely to benefit from added resources, compared to small models that can converge relatively fast. Some models are just not going to benefit from adding more GPU servers beyond a threshold due to their inherent scalability limitations. 
