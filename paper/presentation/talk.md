1. Hi! Hope everyone's having a great day so far. I'm Shijian, today I'm going to present our paper Speeding Up Deep Learning With Transient Servers. This is a collaboration of me, Robert, Lijie and Tian. Lijie is from Chinese Academy of Sciences, and the rest of us are from Worcester Polytechnic Institute. *Pause* Deep learning is great, I think everyone can agree on that. Some people call it magic for a reason. Training a network however, requires a lot of resources. Luckily, nowadays we have cloud platforms we can use so you don't have to buy your own expensive hardwares. But public cloud is not free, if you're not managing it carefully, you'd be hoping that you had bought your own servers.

2. So I want to cut straight to the point, and this is the question we ask ourselves: how do we effectively use cheap but unreliable transient cloud servers for deep learning, specifically within the context of distributed training. Now this is a very broad question, so we broke it down to different parts. And this paper tackles two of the very important steps. First, we need to understand the impact of using transient servers to do distributed training. And then, we need to identify the limitations of both current deep learning frameworks and cloud infrastucture. After this, we need to have potential solutions and improvements for these limitations. I have mentioned two of the key aspects of this paper: distributed training and transient servers. Now I'll talk about the background of them.

4. Why do we need distributed training? The most obvious reason is that training on a single machine is painfully slow. Even with hardware accelerators such as GPU, training a large dataset on a big network can still take days to finish on a single machine. In our experiments we trained a moderate-sized dataset on a moderate-sized network with a pretty powerful GPU, and it still took over 4 hours to finish on a single server. When we switched to train using 4 GPU servers, it scaled linearly to about 1 hour. And that's training for 1 time. In reality, training usually happens many times. For example, in continuous learning, you train the same network with new data. In hyper parameter tuning, you might train the same data on the same network. So really, that's why we need distributed training.

5. With that said, let me briefly talk about the distributed training structure we are using, which this is a very popular and supported natively by TensorFlow. Obviously, we have multiple machines, and all the machines will be grouped into two classes. One is called parameter servers. As the name suggests, parameter servers store the model weights and metadata. They usually do very little actual work. Basically they are serving as the central hub of the communication among machines. The second class, the workers, they are doing the bulk of the work. So usually they are the machines that you will want to attach accelerators to. In this structure, each worker can work independently in an asynchronous way. In each training iteration, the worker pulls the parameters from the parameter server, and does the training (your usual forward pass and back-propagation) to get the gradients. Then they will push the gradients back to parameter server, which will in turn update the weights using the gradients. Every worker follows this flow. Since they are the more powerful and more expensive, they are the perfect target to cut corners.

6. So how do we cut corners? We cut it using transient cloud servers. These are a type of cloud resources that are much cheaper than their on-demand counterparts. However the catch is that they can be revoked by the could providers at any given time. There's no scheduled revocation, there's only a very short time window of warning. Transient servers are actively supported by the major cloud providers. For example in Amazon's case it is called Spot instances and for Google it's called preemptible VMs.

7. What implications do these characteristics have on distributed training? Well since we can configure transient servers to be just as powerful as on-demand servers, we are saving lot of money. As advertised we can save up to 70%. However, since the servers can go down at any moment, undesirable performance degradation might happen. For example, it could take longer for training to finish, or maybe the training just flat fails.

8. Now where are we? Remember that we have two important questions. The first one is, what impact do we have when we use distributed training with transient servers? To answer this, we conducted a large scale empirical study. Let's dive in deeper.

9. Using Google Cloud and TensorFlow, we trained Cifar-10, one of the most used dataset in deep learning research, on ResNet-32, one of the very famous ResNet variations. We mirrored the training configurations from the original ResNet paper, using the same set of hyper parameters and trained to 64K steps. In our study, we mainly care about three performance metrics: training time, total cost of training, and the converged network accuracy. So a good performance would be having short training time, low training cost, and high accuracy. We tested out against several cluster sizes. And our first observation is that without revocation, transient clusters offer the same performance, but the training cost is greatly reduced. Here we are looking at transient clusters that had no revocation during the training session, denoted as r=0, and compare it to the performance of on-demand clusters. As we can see in the columns, there's no significant difference in training time and accuracy. Left value is mean, right is variance. Moving to 4 worker clusters, training time and accuracy stay roughly the same. When we have 8 workers in the cluster, again, there's no degradation in training time and accuracy. But when we look at cost department in all three settings, we can have as much as 64% cost saving.

10. Our second observation is that cluster size, the number of revocations, and the specific time of each revocation can all significantly impact training performance. Here, we take into consideration different revocation scenarios. Same cluster size of 2, 4 and 8. This first row is a rehash of what we saw before. And as the cluster size increases, training time scales almost linearly, the cost of using more servers surprisingly goes down, and the trade-off is that the accuracy drops non-linearly. Notice that this is the artifact from distributed training, not the inclusion of transient servers. Now for 1 revocation scenario, if we look at the overhead section, the smaller the cluster size is, the larger the overhead becomes. For 2 workers this results in 61.7% of more training time. You are spending both more money and more time. Good news is that accuracy stays relatively stable. Same can be said to 2 revocation scenario, 2 worker cluster of course didn't even finish training. And the overhead is even larger for 4 and 8 worker cluster. And the revocation number have noticeable impact on accuracy for larger clusters. 

11. Also, timestamp for each individual revocation matters. Here is a heat map of the normalized lifetime of 4 worker clusters. Workers are labeled one to 4, we have the three metrics we care about here. And the greyscale represents how long the VM lasted during the training. The whiter it is, the earlier it got revoked. So pitch black means it lived to the completion of training. Let's just look at the example of 2 revocations. As it shows, the earlier servers get revoked, the more time and cost overhead it will incur. However, it does seem that time of revocation doesn't impact accuracy as much.

12. Larger cluster size also has other impacts. Our third observation is that the larger the cluster is, the more robust it is to revocations. So we have our clusters from before, and we throw in some other types of GPUs for comparison. P100 and V100 are Nvidia GPUs that are newer and even more powerful than K80 GPU, and also more expensive. In order to keep the cost comparable, we chose to do single machine training with them. Right off the bat, we can see the more powerful GPUs have higher revocation rate. And because they are single machines, revocation means interrupted training. But for the clusters, revocation doesn't necessarily mean the end of training, in fact, we observed 0.6% failure rate for distributed training. As shown here, larger cluster size has smaller variance for training time and cost, representing a more stable and predictable training progress.

13. And a fourth observation made from this is that since using transient servers saves us money, we can use that money to acquire more servers, or more powerful servers, thus achieve faster training. 

14. With the observations, we then ask ourselves the second question: how do we redesign existing deep learning frameworks and revocation mechanisms to provide better support for distributed training on transient servers? There is a huge room for improvement, and we have identified several interesting opportunities.

15. The first opportunity we identified is that a more resilient checkpoint system is needed. Just to quickly introduce checkpoints in distributed training. What are the checkpoints? In TensorFlow's case, they are essentially the model weights and training metadata generated by a special worker called the chief. They provide the basic fault tolerence, allowing you to restart from a checkpoint when the training ends prematurely. Since the chief is in charge of generateing all the checkpoints, it becomes an extra critical point in the system. Remember this example of 4 worker clusters, there's one instance of failed training. It failed because the chief worker got taken down near the end of target finish line. All the other workers finished training but basically wasted all their computation.

16. Alternatively, we could have different revocation policies from the cloud providers. Here is a lifetime CDF of all the different types of transient servers we used. We can see there are several jumps in revocation events where multiple servers got revoked at the same time. Right now all the revocation are seemingly randomly selected with a slight bias towards newly created ones. Imagine the scenario where we have a different revocation policy. For example a selective policy based on user tags. When you create multiple VMs of the same type, you could assign them to different priority tiers. And when revocation happens Google always find eligible candidate from low priority ones. SO you can reduce the chance of critical failure to minimum.

17. Opportunity 3, current deep learning frameworks need support for dynamic training configurations. So the configurations here are basically hyper parameters. They need to be able to adjust to dynamic factors such as cluster size. We observed that under the same hyperparameter set, removing a worker doesn't really impact model accuracy. But what about dynamically adding workers? You might want to do it, for example if you are using EC2. The EC2 spot instances have dynamic pricing so you happend to see a really cheap price. Here is an empirical study. The black line is the baseline of 1 worker and 1 parameter server from start to finish. Grey line represents adding 1 more worker at 16k, 32k and 48k steps but with the hyper parameter unchanged. We can see static hyperparameter leads to accuracy degradation. So red line is the modified version, where we scale the learning rate of training according to the currently active workers. As it turned out, the accuracy drop is mitigated by this method.

18. Another opportunity for the cloud provider side is that improved network communication between datacenters can really benefit utilizing multi-region resources. For some reasons, maybe due to region availablity, the limited capacity pool of resources in each region, or enticing price saving, you might end up with servers in different regions. We tested out the scenario of distributed training on geographically heterogeneous cluster. In this setting, the numbers in the parentheses represents the number of workers in region A, B and C respectively. The parameter server is located in region A. The latency between servers in different regions is that region C has the highest latency, followed by region B then region A. We then compare the performance to that of a region A cluster. Here's the normalized result. We can see heterogenuity incurs no accuracy penalty. But there's quite significant amount of overhead in terms of training time. The higher latency there is, the slower it gets, making it challenging to utilize multi-region servers effectively. So again, better network infrastructure, or alternatively a better communication protocol on the framework side could greatly help with this situation. 

19. To conclude my talk.

20. We answered the question of is it feasible and beneficial to do distributed training on transient servers, with large scale experiments and performance analysis. 

21. We found out that when utilized properly, these cheap but unreliable servers can speed up the training with quite considerable monetary savings.

22. But the volatile nature of transient server revocation can bring along complications. We identified several redesign opportunities and optimization needs to give better support, from both the perspective of current deep learning frameworks, and cloud infrastructure. These serve as a solid foundation for the followup work that we are currently doing. We want to integrate these redesigns into an overarching system to automate and optimize for training acceleration with transient servers.

23. That's the end of my talk, thank you so much for your attention, and I'll happily taking questions.

24. Here's a quick visual reminder of what I just talked.
